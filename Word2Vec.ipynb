{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pedro/anaconda3/envs/condaVENV/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as word2vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"movie_review_dataset\"\n",
    "\n",
    "temp = []\n",
    "for folder in os.listdir(dataset):\n",
    "    folder = os.path.join(dataset, folder)\n",
    "    if not folder.endswith(\".txt\"):\n",
    "        for file in os.listdir(os.path.join(folder, 'pos')):\n",
    "            with open(os.path.join(folder, 'pos', file), 'r', encoding='utf8') as f:\n",
    "                text = f.read()\n",
    "                temp.append([text, True, os.path.join(folder, 'pos', file)])\n",
    "\n",
    "        for file in os.listdir(os.path.join(folder, 'neg')):\n",
    "            with open(os.path.join(folder, 'neg', file), 'r', encoding='utf8') as f:\n",
    "                text = f.read()\n",
    "                temp.append([text, False, os.path.join(folder, 'neg', file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It would be something to try and tell someone ...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/3091_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I gave this show a chance because of Jaleel Wh...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4918_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How good is this film? Apparently, good enough...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/9128_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is great. Stylish, fun, good acting...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/7252_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one of my most favorite movies of all ...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4539_10.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment  \\\n",
       "0  It would be something to try and tell someone ...       True   \n",
       "1  I gave this show a chance because of Jaleel Wh...       True   \n",
       "2  How good is this film? Apparently, good enough...       True   \n",
       "3  This movie is great. Stylish, fun, good acting...       True   \n",
       "4  This is one of my most favorite movies of all ...       True   \n",
       "\n",
       "                                         File  \n",
       "0  movie_review_dataset/part1/pos/3091_10.txt  \n",
       "1   movie_review_dataset/part1/pos/4918_8.txt  \n",
       "2   movie_review_dataset/part1/pos/9128_8.txt  \n",
       "3  movie_review_dataset/part1/pos/7252_10.txt  \n",
       "4  movie_review_dataset/part1/pos/4539_10.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(temp,columns=['Review', 'Sentiment', 'File']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      "Review       50000 non-null object\n",
      "Sentiment    50000 non-null bool\n",
      "File         50000 non-null object\n",
      "dtypes: bool(1), object(2)\n",
      "memory usage: 830.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part2/neg/674_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Review Sentiment  \\\n",
       "count                                               50000     50000   \n",
       "unique                                              49582         2   \n",
       "top     Loved today's show!!! It was a variety and not...      True   \n",
       "freq                                                    5     25000   \n",
       "\n",
       "                                            File  \n",
       "count                                      50000  \n",
       "unique                                     50000  \n",
       "top     movie_review_dataset/part2/neg/674_3.txt  \n",
       "freq                                           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4420_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4422_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4418_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4419_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7960</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>True</td>\n",
       "      <td>movie_review_dataset/part1/pos/4417_10.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  Sentiment  \\\n",
       "964   Loved today's show!!! It was a variety and not...       True   \n",
       "2878  Loved today's show!!! It was a variety and not...       True   \n",
       "5079  Loved today's show!!! It was a variety and not...       True   \n",
       "7192  Loved today's show!!! It was a variety and not...       True   \n",
       "7960  Loved today's show!!! It was a variety and not...       True   \n",
       "\n",
       "                                            File  \n",
       "964   movie_review_dataset/part1/pos/4420_10.txt  \n",
       "2878  movie_review_dataset/part1/pos/4422_10.txt  \n",
       "5079  movie_review_dataset/part1/pos/4418_10.txt  \n",
       "7192  movie_review_dataset/part1/pos/4419_10.txt  \n",
       "7960  movie_review_dataset/part1/pos/4417_10.txt  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Review'].str.contains(\"Loved today's show!!!\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f84a0e21a58>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEMCAYAAAAvaXplAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+T/BQ1AU3dNb+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84a0e1d518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts().plot(kind='bar', color=['blue', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build new Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "i =0 \n",
    "for review in df['Review']:\n",
    "    if i % 1000 == 0:\n",
    "        print('Appended {}/{}'.format(i, len(df['Review'])))\n",
    "    corpus_raw += review\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This one did exactly that.\n",
      "['This', 'one', 'did', 'exactly', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 11,908,918 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_word_count = 3\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "context_size = 7\n",
    "downsampling = 1e-3\n",
    "seed = 1\n",
    "\n",
    "word2vec_model = word2vec.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "word2vec_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 488666\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", word2vec_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45080838"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.train(sentences=sentences, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "\n",
    "word2vec_model.save(os.path.join(\"trained\", \"word2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = word2vec.Word2Vec.load(os.path.join(\"trained\", \"word2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.8927277326583862),\n",
       " ('lady', 0.8728426098823547),\n",
       " ('prostitute', 0.8376454710960388),\n",
       " ('librarian', 0.8351563811302185),\n",
       " ('whore', 0.8246659636497498),\n",
       " ('nurse', 0.8230742812156677),\n",
       " ('man', 0.8220071792602539),\n",
       " ('tramp', 0.8168631196022034),\n",
       " ('Amudha', 0.7986037135124207),\n",
       " ('stud', 0.7957675457000732)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.838233470916748),\n",
       " ('loner', 0.835779070854187),\n",
       " ('guy', 0.8251751065254211),\n",
       " ('woman', 0.8220072388648987),\n",
       " ('person', 0.8078900575637817),\n",
       " ('soldier', 0.8050024509429932),\n",
       " ('coworker', 0.8015088438987732),\n",
       " ('bloke', 0.797492504119873),\n",
       " ('doctor', 0.7818930745124817),\n",
       " ('monk', 0.780976414680481)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200000\n",
    "max_sequence_length = 300\n",
    "batch_size = 100 # Nao Aumentar\n",
    "num_lstm = 50\n",
    "# num_dense = np.random.randint(100, 150)\n",
    "# rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "# rate_drop_dense = 0.15 + np.random.rand() * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['Review'])\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(df['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences=sequence, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_words, len(tokenizer.word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, num_features))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, num_features, weights=[embedding_matrix], trainable=False, input_length=max_sequence_length))\n",
    "model.add(LSTM(num_lstm, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(data)\n",
    "temp['Y'] = df['Sentiment']\n",
    "\n",
    "X = temp.iloc[:, :-1].values\n",
    "Y = temp.iloc[:, -1].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 109s 3ms/step - loss: 0.2805 - acc: 0.8821 - val_loss: 0.2481 - val_acc: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84854cc5c0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 42s 836us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24768681122362612, 0.89706000244617468]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data, df['Sentiment'],batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python3_condaVENV",
   "language": "python",
   "name": "python3_condavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
